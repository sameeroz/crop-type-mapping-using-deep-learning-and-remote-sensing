{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332379a-5eaf-4e3c-a0cb-8835d148e213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tensorflow/examples.git\n",
      "  Cloning https://github.com/tensorflow/examples.git to /tmp/pip-req-build-5te3xnbu\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/examples.git /tmp/pip-req-build-5te3xnbu\n",
      "  Resolved https://github.com/tensorflow/examples.git to commit fff4bcda7201645a1efaea4534403daf5fc03d42\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from tensorflow-examples==0.1703207612.1461250479831370929614362828255168868146460245314) (2.0.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from tensorflow-examples==0.1703207612.1461250479831370929614362828255168868146460245314) (1.16.0)\n",
      "Building wheels for collected packages: tensorflow-examples\n",
      "  Building wheel for tensorflow-examples (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorflow-examples: filename=tensorflow_examples-0.1703207612.1461250479831370929614362828255168868146460245314-py3-none-any.whl size=301582 sha256=b2a2e805de407a62995e8ffa72635295226b4e5803e50e4e7447ed3b649249cd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vd5f_yo2/wheels/72/5f/d0/7fe769eaa229bf20101d11a357eb23c83c481bee2d7f710599\n",
      "Successfully built tensorflow-examples\n",
      "Installing collected packages: tensorflow-examples\n",
      "Successfully installed tensorflow-examples-0.1703207612.1461250479831370929614362828255168868146460245314\n",
      "Collecting tfds-nightly\n",
      "  Downloading tfds_nightly-4.9.4.dev202403240044-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from tfds-nightly) (2.0.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tfds-nightly) (8.1.7)\n",
      "Collecting dm-tree (from tfds-nightly)\n",
      "  Downloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Collecting etils>=1.6.0 (from etils[enp,epath,epy,etree]>=1.6.0->tfds-nightly)\n",
      "  Downloading etils-1.7.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting immutabledict (from tfds-nightly)\n",
      "  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tfds-nightly) (1.26.2)\n",
      "Collecting promise (from tfds-nightly)\n",
      "  Using cached promise-2.3-py3-none-any.whl\n",
      "Requirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.10/site-packages (from tfds-nightly) (4.21.12)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from tfds-nightly) (5.9.5)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from tfds-nightly) (12.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from tfds-nightly) (2.31.0)\n",
      "Collecting tensorflow-metadata (from tfds-nightly)\n",
      "  Using cached tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from tfds-nightly) (2.3.0)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tfds-nightly) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from tfds-nightly) (4.66.1)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from tfds-nightly) (1.16.0)\n",
      "Collecting array-record>=0.5.0 (from tfds-nightly)\n",
      "  Using cached array_record-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (503 bytes)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0->tfds-nightly) (4.5.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0->tfds-nightly) (2023.6.0)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0->tfds-nightly) (6.1.1)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0->tfds-nightly) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tfds-nightly) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tfds-nightly) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tfds-nightly) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tfds-nightly) (2023.11.17)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from promise->tfds-nightly) (1.16.0)\n",
      "Collecting absl-py (from tfds-nightly)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-metadata->tfds-nightly) (1.62.0)\n",
      "Collecting protobuf>=3.20 (from tfds-nightly)\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Downloading tfds_nightly-4.9.4.dev202403240044-py3-none-any.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached array_record-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Downloading etils-1.7.0-py3-none-any.whl (152 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "Downloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\n",
      "Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Installing collected packages: dm-tree, protobuf, promise, immutabledict, etils, absl-py, tensorflow-metadata, array-record, tfds-nightly\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.21.12\n",
      "    Uninstalling protobuf-4.21.12:\n",
      "      Successfully uninstalled protobuf-4.21.12\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 2.0.0\n",
      "    Uninstalling absl-py-2.0.0:\n",
      "      Successfully uninstalled absl-py-2.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.198.0 requires boto3<2.0,>=1.29.6, but you have boto3 1.28.64 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-1.4.0 array-record-0.5.0 dm-tree-0.1.8 etils-1.7.0 immutabledict-4.2.0 promise-2.3 protobuf-3.20.3 tensorflow-metadata-1.14.0 tfds-nightly-4.9.4.dev202403240044\n",
      "Collecting focal-loss\n",
      "  Downloading focal_loss-0.0.7-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: tensorflow>=2.2 in /opt/conda/lib/python3.10/site-packages (from focal-loss) (2.12.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (3.10.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (0.4.20)\n",
      "Requirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (1.26.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (1.54.3)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2->focal-loss) (2.12.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow>=2.2->focal-loss) (0.42.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow>=2.2->focal-loss) (0.3.1)\n",
      "Requirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow>=2.2->focal-loss) (1.11.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2->focal-loss) (2.25.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2->focal-loss) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2->focal-loss) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2->focal-loss) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2->focal-loss) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2->focal-loss) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal-loss) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal-loss) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal-loss) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal-loss) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal-loss) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal-loss) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal-loss) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal-loss) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal-loss) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal-loss) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal-loss) (3.2.2)\n",
      "Using cached focal_loss-0.0.7-py3-none-any.whl (19 kB)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q rasterio\n",
    "!pip install -q geopandas\n",
    "!pip install git+https://github.com/tensorflow/examples.git\n",
    "!pip install -U tfds-nightly\n",
    "!pip install focal-loss\n",
    "!pip install tensorflow-addons\n",
    "!pip install tensorflow==2.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b8f62cc-9031-4a96-a2d6-c027080f890c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 20:39:31.960067: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-30 20:39:32.010117: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-30 20:39:32.011106: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-30 20:39:33.080407: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, glob, functools, fnmatch\n",
    "from zipfile import ZipFile\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "mpl.rcParams['figure.figsize'] = (12, 12)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import rasterio\n",
    "from rasterio import features, mask, windows\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers, losses, models\n",
    "from tensorflow.python.keras import backend as K  \n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "# Resnet importation ------------------------------\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# ------------------------------\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca0024-23c6-4cd0-be6a-b2c2d49a9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'images/'\n",
    "sentinel_timestamps = ['2017-03-22', '2017-05-31', '2017-06-20', '2017-08-04']\n",
    "test_sentinel_timestamp = ['2017-07-10']\n",
    "target_crs = 'epsg:32734'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11e202-8d49-4784-9c32-94e8b717930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_index = pd.read_csv('crop_id_list.csv')\n",
    "class_names = class_index.crop.unique()\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b3923-44ad-43e5-8c33-b4ad173b38a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentinel_read(sentinel_timestamp):\n",
    "    sentinel_dir = os.path.join(root_dir, sentinel_timestamp)\n",
    "    print(sentinel_dir+'/**/*.jp2')\n",
    "    bands = glob.glob(sentinel_dir+'/**/*.jp2', recursive=True)\n",
    "    # Read band metadata and arrays\n",
    "    # metadata\n",
    "    src_2 = rasterio.open(fnmatch.filter(bands, '*B02.jp2')[0]) #blue\n",
    "    src_3 = rasterio.open(fnmatch.filter(bands, '*B03.jp2')[0]) #green\n",
    "    src_4 = rasterio.open(fnmatch.filter(bands, '*B04.jp2')[0]) #red\n",
    "    src_8 = rasterio.open(fnmatch.filter(bands, '*B08.jp2')[0]) #near infrared\n",
    "\n",
    "    # array\n",
    "    arr_2 = src_2.read()\n",
    "    arr_3 = src_3.read()\n",
    "    arr_4 = src_4.read()\n",
    "    arr_8 = src_8.read()\n",
    "    return sentinel_dir, arr_2, arr_3, arr_4, arr_8, src_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a76e6-8aa0-4e4e-a5a5-379bccd6c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexnormstack(red, nir):\n",
    "    \n",
    "    def NDIcalc(nir, red): \n",
    "        ndi = (nir - red) / (nir  + red + 1e-5) \n",
    "        return ndi\n",
    "    \n",
    "    def WRDVIcalc(red,nir):\n",
    "        a = 0.2\n",
    "        wdrvi = (a * nir - red) / (a * nir + red)\n",
    "        return wdrvi\n",
    "    \n",
    "    def SAVIcalc(red, nir):\n",
    "        savi = 1.5 * (nir - red) / (nir + red + 0.5)\n",
    "        return savi\n",
    "    \n",
    "    def norm(arr):\n",
    "        arr_norm = (255*(arr - np.min(arr))/np.ptp(arr)) \n",
    "        return arr_norm\n",
    "    \n",
    "    ndvi = NDIcalc(nir,red) \n",
    "    \n",
    "    savi = SAVIcalc(red,nir) \n",
    "    \n",
    "    wdrvi = WRDVIcalc(red,nir)\n",
    "\n",
    "    ndvi = ndvi.transpose(1,2,0)\n",
    "    savi = savi.transpose(1,2,0)\n",
    "    wdrvi = wdrvi.transpose(1,2,0)\n",
    "\n",
    "    index_stack = np.dstack((ndvi, savi, wdrvi))\n",
    "    \n",
    "    return index_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bb4463-62d4-429d-b429-c167ae428aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(geo, src_8):\n",
    "    geo = gpd.read_file(geo)\n",
    "    # check for and remove invalid geometries\n",
    "    geo = geo.loc[geo.is_valid] \n",
    "    # reproject training data into local coordinate reference system\n",
    "    geo = geo.to_crs(crs={'init': target_crs})\n",
    "    #convert the class identifier column to type integer\n",
    "    geo['Crop_Id_Ne_int']  = geo.Crop_Id_Ne.astype(int)\n",
    "    # pair the geometries and their integer class values\n",
    "    shapes = ((geom,value) for geom, value in zip(geo.geometry, geo.Crop_Id_Ne_int)) \n",
    "    # get the metadata (height, width, channels, transform, CRS) to use in constructing the labeled image array\n",
    "    src_8_prf = src_8.profile\n",
    "    # construct a blank array from the metadata and burn the labels in\n",
    "    labels = features.rasterize(shapes=shapes, out_shape=(src_8_prf['height'], src_8_prf['width']), fill=0, all_touched=True, transform=src_8_prf['transform'], dtype=src_8_prf['dtype'])\n",
    "    print(\"Values in labeled image: \", np.unique(labels))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1d01c-24e2-4c63-9f2a-3f0c4b5238aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(sentinel_dir, index_stack, labels, src_8):\n",
    "    index_stack = (index_stack * 255).astype(np.uint8)\n",
    "    index_stack_t = index_stack.transpose(2, 0, 1)\n",
    "    labels = labels.astype(np.uint8)\n",
    "\n",
    "    index_stack_out = rasterio.open(sentinel_dir+'/index_stack.tiff', 'w', driver='Gtiff',\n",
    "                              width=src_8.width, height=src_8.height,\n",
    "                              count=3,\n",
    "                              crs=src_8.crs,\n",
    "                              transform=src_8.transform,\n",
    "                              dtype='uint8')\n",
    "\n",
    "    index_stack_out.write(index_stack_t)\n",
    "    index_stack_out.close()\n",
    "\n",
    "    labels_out=rasterio.open(sentinel_dir+'/labels.tiff', 'w', driver='Gtiff',\n",
    "                              width=src_8.width, height=src_8.height,\n",
    "                              count=1,\n",
    "                              crs=src_8.crs,\n",
    "                              transform=src_8.transform,\n",
    "                              dtype='uint8')\n",
    "\n",
    "    labels_out.write(labels, 1)\n",
    "    labels_out.close()\n",
    "    \n",
    "    return sentinel_dir+'/index_stack.tiff', sentinel_dir+'/labels.tiff'\n",
    "#Now let's divide the Sentinel 2 index stack and labeled image into 224x224 pixel tiles\n",
    "\n",
    "def tile(index_stack, labels, sentinel_timestamp):\n",
    "    tiles_dir = root_dir+'tiled/'\n",
    "    img_dir = root_dir+'tiled/images/'\n",
    "    label_dir = root_dir+'tiled/labels/'\n",
    "    dirs = [tiles_dir, img_dir, label_dir]\n",
    "    for d in dirs:\n",
    "        if not os.path.exists(d):\n",
    "            os.makedirs(d)\n",
    "    \n",
    "    # set tile height and width\n",
    "    height,width = 224, 224\n",
    "    \n",
    "    def get_tiles(ds, width=224, height=224):\n",
    "        # get number of rows and columns (pixels) in the entire input image\n",
    "        nols, nrows = ds.meta['width'], ds.meta['height']\n",
    "        # get the grid from which tiles will be made \n",
    "        offsets = product(range(0, nols, width), range(0, nrows, height))\n",
    "        # get the window of the entire input image\n",
    "        big_window = windows.Window(col_off=0, row_off=0, width=nols, height=nrows)\n",
    "        # tile the big window by mini-windows per grid cell\n",
    "        for col_off, row_off in offsets:\n",
    "            window = windows.Window(col_off=col_off, row_off=row_off, width=width, height=height).intersection(big_window)\n",
    "            transform = windows.transform(window, ds.transform)\n",
    "            yield window, transform\n",
    "      \n",
    "    tile_width, tile_height = 224, 224\n",
    "    \n",
    "    def crop(inpath, outpath, c):\n",
    "        # read input image\n",
    "        image = rasterio.open(inpath)\n",
    "        # get the metadata \n",
    "        meta = image.meta.copy()\n",
    "        # set the number of channels to 3 or 1, depending on if its the index image or labels image\n",
    "        meta['count'] = int(c)\n",
    "        # set the tile output file format to PNG (saves spatial metadata unlike JPG)\n",
    "        meta['driver']='PNG'\n",
    "        # tile the input image by the mini-windows\n",
    "        i = 0\n",
    "        for window, transform in get_tiles(image):\n",
    "            meta['transform'] = transform\n",
    "            meta['width'], meta['height'] = window.width, window.height\n",
    "            outfile = outpath+\"tile_%s_%s.png\" % (sentinel_timestamp, str(i))\n",
    "            with rasterio.open(outfile, 'w', **meta) as outds:\n",
    "                outds.write(image.read(window=window))\n",
    "            i = i+1\n",
    "            \n",
    "    def process_tiles(index_flag):\n",
    "        # tile the input images, when index_flag == True, we are tiling the spectral index image, \n",
    "        # when False we are tiling the labels image\n",
    "        if index_flag==True:\n",
    "            inpath = sentinel_dir+'/index_stack.tiff'\n",
    "            outpath=img_dir\n",
    "            crop(inpath, outpath, 3)\n",
    "        else:\n",
    "            inpath = sentinel_dir+'/labels.tiff'\n",
    "            outpath=label_dir\n",
    "            crop(inpath, outpath, 1)\n",
    "                \n",
    "    process_tiles(index_flag=True) # tile index stack\n",
    "    process_tiles(index_flag=False) # tile labels\n",
    "    return tiles_dir, img_dir, label_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f0db3-382c-4734-95d7-2eea1057b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "\n",
    "# # Path to the zip file containing Sentinel-SAFE data\n",
    "# for root, dirs, files in os.walk(\"images\"):\n",
    "#     for file_name in files:\n",
    "#         if file_name.endswith('.zip'):\n",
    "#             zip_file_path = 'images/' + file_name\n",
    "#             extracted_dir = 'images/' + file_name[:-4]\n",
    "\n",
    "#             with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "#                 zip_ref.extractall(extracted_dir)\n",
    "#             print(file_name)\n",
    "\n",
    "# Find and open band 2 data within the extracted directory\n",
    "# for root, dirs, files in os.walk(extracted_dir):\n",
    "#     for file_name in files:\n",
    "#         if file_name.endswith('_B01.jp2'):\n",
    "#             band2_path = os.path.join(root, file_name)\n",
    "#             with rasterio.open(band2_path) as dataset:\n",
    "#                 # Read metadata\n",
    "#                 print(dataset.profile)\n",
    "                \n",
    "#                 # Read band 2 data\n",
    "#                 band2_data = dataset.read(1)  # Assuming band 2 is the first band (index 1-based)\n",
    "                # If you need to apply any transformation to the data (e.g., scaling), you can do it here\n",
    "\n",
    "# Now you have band 2 data in the 'band2_data' variable, which you can use for further processing or visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bfbf08-25ab-4c68-b2c2-4d8baa58c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to write the files out to your personal drive,\n",
    "# set write_out = True, but I recommend trying \n",
    "# that in your free time because it takes about 2 hours for all 4 training Sentinel timetamps, \n",
    "# and about 30 minutes for the test timestamp.\n",
    "\n",
    "write_out = True\n",
    "if write_out is True:\n",
    "  for timestamp in sentinel_timestamps:\n",
    "    # timestamp, tiles_dir, img_dir, label_dir = main(timestamp)\n",
    "    print(\"timestamp: \", timestamp)\n",
    "\n",
    "    sentinel_dir, arr_2, arr_3, arr_4, arr_8, src_8 = sentinel_read(timestamp)\n",
    "\n",
    "    # Calculate indices and combine the indices into one single 3 channel image\n",
    "\n",
    "    index_stack = indexnormstack(arr_4, arr_8)\n",
    "\n",
    "    # Rasterize labels\n",
    "    labels = label('train/train.shp', src_8)\n",
    "\n",
    "    # reset the path of the Sentinel 2 image directory to your personal drive \n",
    "    # if you want to write the files out\n",
    "    sentinel_dir = os.path.join(\"res\", timestamp)\n",
    "    if not os.path.exists(sentinel_dir):\n",
    "        os.makedirs(sentinel_dir)\n",
    "    print(sentinel_dir, src_8)\n",
    "\n",
    "    # Save index stack and labels to geotiff\n",
    "    index_stack_file, labels_file = save_images(sentinel_dir, index_stack, labels, src_8)\n",
    "\n",
    "    # Tile images into 224x224\n",
    "    tiles_dir, img_dir, label_dir = tile(index_stack, labels, timestamp)\n",
    "    print(\"done\")\n",
    "else:\n",
    "    print(\"Not writing to file; using data in shared drive too.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35760781-6871-48d6-bda0-ae6a44c69b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the paths to point to the tiles images\n",
    "tiles_dir = root_dir+'tiled/'\n",
    "\n",
    "# train\n",
    "img_dir = root_dir+'tiled/images/'\n",
    "label_dir = root_dir+'tiled/labels/'\n",
    "\n",
    "# test\n",
    "test_img_dir = root_dir+'tiled/images_test/'\n",
    "test_label_dir = root_dir+'tiled/labels_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf77e4-4679-4bd1-9eb0-6aa21aa67c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get initial counts of train and test images\n",
    "%ls $img_dir | wc -l\n",
    "%ls $test_img_dir | wc -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb3f74-00b2-4779-901b-fe9e43df88ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_train_test_lists(imdir, lbldir):\n",
    "  imgs = glob.glob(imdir+\"/*.png\")\n",
    "  print(imgs)\n",
    "  dset_list = []\n",
    "  for img in imgs:\n",
    "    filename_split = os.path.splitext(img) \n",
    "    filename_zero, fileext = filename_split \n",
    "    basename = os.path.basename(filename_zero) \n",
    "    dset_list.append(basename)\n",
    "    \n",
    "  x_filenames = []\n",
    "  y_filenames = []\n",
    "  for img_id in dset_list:\n",
    "    x_filenames.append(os.path.join(imdir, \"{}.png\".format(img_id)))\n",
    "    y_filenames.append(os.path.join(lbldir, \"{}.png\".format(img_id)))\n",
    "    \n",
    "  print(\"number of images: \", len(dset_list))\n",
    "  return dset_list, x_filenames, y_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac75244-4eba-427f-9e3c-e6577c191f52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_list, x_train_filenames, y_train_filenames = get_train_test_lists(img_dir, label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59becd14-9d6d-4d6d-909a-eed1a3295f75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_list, x_test_filenames, y_test_filenames = get_train_test_lists(test_img_dir, test_label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0161173f-d842-4856-bcdf-493073e46ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = False\n",
    "\n",
    "if not skip:\n",
    "  background_list_train = []\n",
    "  for i in train_list: \n",
    "      # read in each labeled images\n",
    "      img = np.array(Image.open(label_dir+\"{}.png\".format(i))) \n",
    "      # check if no values in image are greater than zero (background value)\n",
    "      if img.max()==0:\n",
    "          background_list_train.append(i)\n",
    "          \n",
    "  print(\"number of background training images: \", len(background_list_train))\n",
    "\n",
    "  background_list_test = []\n",
    "  for i in test_list: \n",
    "      img = np.array(Image.open(test_label_dir+\"{}.png\".format(i))) \n",
    "      if img.max()==0:\n",
    "          background_list_test.append(i)\n",
    "          \n",
    "  print(\"number of background test images: \", len(background_list_test))\n",
    "\n",
    "  with open(os.path.join(\"\",'background_list_train.txt'), 'w') as f:\n",
    "    for item in background_list_train:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "  with open(os.path.join(\"\",'background_list_test.txt'), 'w') as f:\n",
    "    for item in background_list_test:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "else:\n",
    "  background_list_train = [line.strip() for line in open(\"background_list_train.txt\", 'r')]\n",
    "  print(\"number of background training images: \", len(background_list_train))\n",
    "  \n",
    "  background_list_test = [line.strip() for line in open(\"background_list_test.txt\", 'r')]\n",
    "  print(\"number of background test images: \", len(background_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3321f6ac-81ca-4c1a-9d4e-7a56ad4cef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_removal = len(background_list_train) * 0.9\n",
    "train_list_clean = [y for y in train_list if y not in background_list_train[0:int(background_removal)]]\n",
    "\n",
    "x_train_filenames = []\n",
    "y_train_filenames = []\n",
    "for img_id in train_list_clean: \n",
    "  x_train_filenames.append(os.path.join(img_dir, \"{}.png\".format(img_id)))\n",
    "  y_train_filenames.append(os.path.join(label_dir, \"{}.png\".format(img_id)))\n",
    "    \n",
    "print(len(train_list_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142fd7ec-a2df-4ae5-b5e6-10a4cac6e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_filenames, x_val_filenames, y_train_filenames, y_val_filenames = train_test_split(x_train_filenames, y_train_filenames, test_size=0.1, random_state=42)\n",
    "\n",
    "num_train_examples = len(x_train_filenames)\n",
    "num_val_examples = len(x_val_filenames)\n",
    "num_test_examples = len(x_test_filenames)\n",
    "\n",
    "print(\"Number of training examples: {}\".format(num_train_examples))\n",
    "print(\"Number of validation examples: {}\".format(num_val_examples))\n",
    "print(\"Number of test examples: {}\".format(num_test_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f108253-6f89-4fe6-88aa-1b5442040928",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_num = 3\n",
    "\n",
    "# select only for tiles with foreground labels present\n",
    "foreground_list_x = []\n",
    "foreground_list_y = []\n",
    "for x,y in zip(x_train_filenames, y_train_filenames): \n",
    "    img = np.array(Image.open(y)) \n",
    "    if img.max()>0:\n",
    "        foreground_list_x.append(x)\n",
    "        foreground_list_y.append(y)\n",
    "\n",
    "num_foreground_examples = len(foreground_list_y)\n",
    "\n",
    "# randomlize the choice of image and label pairs\n",
    "r_choices = np.random.choice(num_foreground_examples, display_num)\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "for i in range(0, display_num * 2, 2):\n",
    "  img_num = r_choices[i // 2]\n",
    "  x_pathname = foreground_list_x[img_num]\n",
    "  y_pathname = foreground_list_y[img_num]\n",
    "  \n",
    "  plt.subplot(display_num, 2, i + 1)\n",
    "  plt.imshow(mpimg.imread(x_pathname))\n",
    "  plt.title(\"Original Image\")\n",
    "  \n",
    "  example_labels = Image.open(y_pathname)\n",
    "  label_vals = np.unique(np.array(example_labels))\n",
    "  \n",
    "  plt.subplot(display_num, 2, i + 2)\n",
    "  plt.imshow(example_labels)\n",
    "  plt.title(\"Masked Image\")  \n",
    "  \n",
    "plt.suptitle(\"Examples of Images and their Masks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b35384-e7fc-412f-947e-32412fc613f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set input image shape\n",
    "img_shape = (224, 224, 3)\n",
    "# set batch size for model\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd046b13-3146-4647-9479-1cb1f1ee13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reading the tiles into TensorFlow tensors \n",
    "# See TensorFlow documentation for explanation of tensor: https://www.tensorflow.org/guide/tensor\n",
    "def _process_pathnames(fname, label_path):\n",
    "  # We map this function onto each pathname pair  \n",
    "  img_str = tf.io.read_file(fname)\n",
    "  img = tf.image.decode_png(img_str, channels=3)\n",
    "\n",
    "  label_img_str = tf.io.read_file(label_path)\n",
    "\n",
    "  # These are png images so they return as (num_frames, h, w, c)\n",
    "  label_img = tf.image.decode_png(label_img_str, channels=1)\n",
    "  # The label image should have any values between 0 and 9, indicating pixel wise\n",
    "  # cropt type class or background (0). We take the first channel only. \n",
    "  label_img = label_img[:, :, 0]\n",
    "  label_img = tf.expand_dims(label_img, axis=-1)\n",
    "  return img, label_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb813e-8f7f-4bc5-8c92-3dae29c3051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to augment the data with horizontal flip\n",
    "def flip_img_h(horizontal_flip, tr_img, label_img):\n",
    "  if horizontal_flip:\n",
    "    flip_prob = tf.random.uniform([], 0.0, 1.0)\n",
    "    tr_img, label_img = tf.cond(tf.less(flip_prob, 0.5),\n",
    "                                lambda: (tf.image.flip_left_right(tr_img), tf.image.flip_left_right(label_img)),\n",
    "                                lambda: (tr_img, label_img))\n",
    "  return tr_img, label_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853e41c-0db2-457d-b83f-89026db81371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to augment the data with vertical flip\n",
    "def flip_img_v(vertical_flip, tr_img, label_img):\n",
    "  if vertical_flip:\n",
    "    flip_prob = tf.random.uniform([], 0.0, 1.0)\n",
    "    tr_img, label_img = tf.cond(tf.less(flip_prob, 0.5),\n",
    "                                lambda: (tf.image.flip_up_down(tr_img), tf.image.flip_up_down(label_img)),\n",
    "                                lambda: (tr_img, label_img))\n",
    "  return tr_img, label_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793cdafd-8ab9-45aa-8e8f-4a309f51ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to augment the images and labels\n",
    "def _augment(img,\n",
    "             label_img,\n",
    "             resize=None,  # Resize the image to some size e.g. [256, 256]\n",
    "             scale=1,  # Scale image e.g. 1 / 255.\n",
    "             horizontal_flip=False,\n",
    "             vertical_flip=False): \n",
    "  if resize is not None:\n",
    "    # Resize both images\n",
    "    label_img = tf.image.resize(label_img, resize)\n",
    "    img = tf.image.resize(img, resize)\n",
    "  \n",
    "  img, label_img = flip_img_h(horizontal_flip, img, label_img)\n",
    "  img, label_img = flip_img_v(vertical_flip, img, label_img)\n",
    "  img = tf.cast(img, tf.float32) * scale  #tf.to_float(img) * scale \n",
    "  #label_img = tf.cast(label_img, tf.float32) * scale\n",
    "  #print(\"tensor: \", tf.unique(tf.keras.backend.print_tensor(label_img)))\n",
    "  return img, label_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37df129-8caa-499d-8df1-93a23ab61383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to tie all of the above four dataset processing functions together \n",
    "def get_baseline_dataset(filenames, \n",
    "                         labels,\n",
    "                         preproc_fn=functools.partial(_augment),\n",
    "                         threads=5, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True):           \n",
    "  num_x = len(filenames)\n",
    "  # Create a dataset from the filenames and labels\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "  # Map our preprocessing function to every element in our dataset, taking\n",
    "  # advantage of multithreading\n",
    "  dataset = dataset.map(_process_pathnames, num_parallel_calls=threads)\n",
    "  if preproc_fn.keywords is not None and 'resize' not in preproc_fn.keywords:\n",
    "    assert batch_size == 1, \"Batching images must be of the same size\"\n",
    "\n",
    "  dataset = dataset.map(preproc_fn, num_parallel_calls=threads)\n",
    "  \n",
    "  if shuffle:\n",
    "    dataset = dataset.shuffle(num_x)\n",
    "  \n",
    "  \n",
    "  # It's necessary to repeat our data for all epochs \n",
    "  dataset = dataset.repeat().batch(batch_size)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa104a9a-9dfd-4820-bc8c-0f31f9537ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset configuration for training\n",
    "tr_cfg = {\n",
    "    'resize': [img_shape[0], img_shape[1]],\n",
    "    'scale': 1 / 255.,\n",
    "    'horizontal_flip': True,\n",
    "    'vertical_flip': True,\n",
    "}\n",
    "tr_preprocessing_fn = functools.partial(_augment, **tr_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9557b36-0771-4ce6-896d-181891e291b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset configuration for validation\n",
    "val_cfg = {\n",
    "    'resize': [img_shape[0], img_shape[1]],\n",
    "    'scale': 1 / 255.,\n",
    "}\n",
    "val_preprocessing_fn = functools.partial(_augment, **val_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd547b6d-f7c8-4874-86b8-c4e8acd66587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset configuration for testing\n",
    "test_cfg = {\n",
    "    'resize': [img_shape[0], img_shape[1]],\n",
    "    'scale': 1 / 255.,\n",
    "}\n",
    "test_preprocessing_fn = functools.partial(_augment, **test_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6c612-145d-456a-ae9c-aaea74e26f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the TensorFlow datasets\n",
    "train_ds = get_baseline_dataset(x_train_filenames,\n",
    "                                y_train_filenames,\n",
    "                                preproc_fn=tr_preprocessing_fn,\n",
    "                                batch_size=batch_size)\n",
    "val_ds = get_baseline_dataset(x_val_filenames,\n",
    "                              y_val_filenames, \n",
    "                              preproc_fn=val_preprocessing_fn,\n",
    "                              batch_size=batch_size)\n",
    "test_ds = get_baseline_dataset(x_test_filenames,\n",
    "                              y_test_filenames, \n",
    "                              preproc_fn=test_preprocessing_fn,\n",
    "                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee45127-ea70-4189-87b6-640901c89969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will display some samples from the datasets\n",
    "display_num = 1\n",
    "r_choices = np.random.choice(num_foreground_examples, 1)\n",
    "for i in range(0, display_num * 2, 2):\n",
    "  img_num = r_choices[i // 2]\n",
    "\n",
    "temp_ds = get_baseline_dataset(foreground_list_x[img_num:img_num+1], \n",
    "                               foreground_list_y[img_num:img_num+1],\n",
    "                               preproc_fn=tr_preprocessing_fn,\n",
    "                               batch_size=1,\n",
    "                               shuffle=False)\n",
    "\n",
    "# Let's examine some of these augmented images\n",
    "\n",
    "iterator = iter(temp_ds)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "batch_of_imgs, label = next_element\n",
    "\n",
    "# Running next element in our graph will produce a batch of images\n",
    "\n",
    "sample_image, sample_mask = batch_of_imgs[0], label[0,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d9c1ae-918c-4b52-8462-a1dd3bdaa225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "  plt.figure(figsize=(15, 15))\n",
    "\n",
    "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce1c98a-8101-41a2-a57b-6f0d6f9ca542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display sample train image\n",
    "display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5238f21e-1bc6-4462-93f4-3318037e5246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the forground list to capture the validation images\n",
    "foreground_list_x = []\n",
    "foreground_list_y = []\n",
    "for x,y in zip(x_val_filenames, y_val_filenames): \n",
    "    img = np.array(Image.open(y)) \n",
    "    if img.max()>0:\n",
    "        foreground_list_x.append(x)\n",
    "        foreground_list_y.append(y)\n",
    "\n",
    "num_foreground_examples = len(foreground_list_y)\n",
    "\n",
    "display_num = 1\n",
    "r_choices = np.random.choice(num_foreground_examples, 1)\n",
    "for i in range(0, display_num * 2, 2):\n",
    "  img_num = r_choices[i // 2]\n",
    "\n",
    "temp_ds = get_baseline_dataset(foreground_list_x[img_num:img_num+1], \n",
    "                               foreground_list_y[img_num:img_num+1],\n",
    "                               preproc_fn=val_preprocessing_fn,\n",
    "                               batch_size=1,\n",
    "                               shuffle=False)\n",
    "\n",
    "# Let's examine some of these augmented images\n",
    "\n",
    "iterator = iter(temp_ds)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "batch_of_imgs, label = next_element\n",
    "\n",
    "# Running next element in our graph will produce a batch of images\n",
    "\n",
    "sample_image, sample_mask = batch_of_imgs[0], label[0, :, :, :]\n",
    "\n",
    "# display sample validation image\n",
    "display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492d7b1-54b1-4792-8878-bc6292722277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the forground list to capture the test images\n",
    "foreground_list_x = []\n",
    "foreground_list_y = []\n",
    "for x,y in zip(x_test_filenames, y_test_filenames): \n",
    "    img = np.array(Image.open(y)) \n",
    "    if img.max()>0:\n",
    "        foreground_list_x.append(x)\n",
    "        foreground_list_y.append(y)\n",
    "\n",
    "num_foreground_examples = len(foreground_list_y)\n",
    "\n",
    "display_num = 1\n",
    "r_choices = np.random.choice(num_foreground_examples, 1)\n",
    "for i in range(0, display_num * 2, 2):\n",
    "  img_num = r_choices[i // 2]\n",
    "\n",
    "temp_ds = get_baseline_dataset(foreground_list_x[img_num:img_num+1], \n",
    "                               foreground_list_y[img_num:img_num+1],\n",
    "                               preproc_fn=test_preprocessing_fn,\n",
    "                               batch_size=1,\n",
    "                               shuffle=False)\n",
    "\n",
    "# Let's examine some of these augmented images\n",
    "\n",
    "iterator = iter(temp_ds)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "batch_of_imgs, label = next_element\n",
    "\n",
    "# Running next element in our graph will produce a batch of images\n",
    "\n",
    "sample_image, sample_mask = batch_of_imgs[0], label[0,:,:,:]\n",
    "\n",
    "# display sample test image\n",
    "display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5475837-c5e4-4290-8b0f-241d027c194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of model output channels to the number of classes (including background)\n",
    "OUTPUT_CHANNELS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39b0cb30-0ea1-4ae9-a4ba-d8cafdae7e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define input shape\n",
    "input_shape = (192, 224, 3)\n",
    "\n",
    "# Load ResNet50 model with pre-trained weights and without the top classification layer\n",
    "base_model = ResNet50(weights='imagenet', input_shape=input_shape, include_top=False)\n",
    "\n",
    "# Use the activations of these layers\n",
    "layer_names = [\n",
    "    'conv1_relu',       # 112x112\n",
    "    'conv2_block3_out', # 56x56\n",
    "    'conv3_block4_out', # 28x28\n",
    "    'conv4_block6_out', # 14x14\n",
    "    'conv5_block3_out', # 7x7\n",
    "]\n",
    "layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "# Create the feature extraction model\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
    "\n",
    "# Freeze the pre-trained layers\n",
    "down_stack.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4339c7d-a60c-49cf-bc40-ab5201f2907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "up_stack = [\n",
    "    tf.keras.layers.UpSampling2D(size=(2, 2)),  # 7x7 -> 14x14 (conv5_block3_out)\n",
    "    tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same'),  # 14x14\n",
    "    tf.keras.layers.UpSampling2D(size=(2, 2)),  # 14x14 -> 28x28 (conv4_block6_out)\n",
    "    tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),  # 28x28\n",
    "    tf.keras.layers.UpSampling2D(size=(2, 2)),  # 28x28 -> 56x56 (conv3_block4_out)\n",
    "    tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),  # 56x56\n",
    "    tf.keras.layers.UpSampling2D(size=(2, 2)),  # 56x56 -> 112x112 (conv2_block3_out)\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),  # 112x112\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e08dfd1-41f5-4f62-8ff6-d42f10872718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def unet_model(output_channels):\n",
    "    inputs = tf.keras.layers.Input(shape=[224, 224, 3])\n",
    "    x = inputs\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = down_stack(x)\n",
    "    x = skips[-1]\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        # Resize the skip tensor to match the spatial dimensions of x\n",
    "        skip = tf.keras.layers.experimental.preprocessing.Resizing(14, 14)(skip)\n",
    "        concat = tf.keras.layers.Concatenate()\n",
    "        x = concat([x, skip])\n",
    "\n",
    "    # This is the last layer of the model\n",
    "    last = tf.keras.layers.Conv2DTranspose(\n",
    "        output_channels, 3, strides=2, padding='same')  # 64x64 -> 224x224\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4a44fca-0065-41e3-97aa-7008c984ea6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sdfsdf\n"
     ]
    }
   ],
   "source": [
    "# model = unet_model(OUTPUT_CHANNELS)\n",
    "print(\"Sdfsdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f192f-78ff-4503-a1bb-efa9c05be180",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Farmpin_training.csv')\n",
    "inv_freq = np.array(1/(train_df.crop_id.value_counts()/len(train_df)))\n",
    "inv_freq = [0.,*inv_freq]\n",
    "class_weights = {0 : inv_freq[0], 1: inv_freq[1], 2: inv_freq[2], 3: inv_freq[3], \n",
    "                4: inv_freq[4], 5: inv_freq[5], 6: inv_freq[6],\n",
    "                7: inv_freq[7], 8: inv_freq[8], 9: inv_freq[9]}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ed1eb-61dc-48b3-8bf5-8e380e08af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss=SparseCategoricalFocalLoss(gamma=2, from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a947ec7-1017-4fa0-8ad8-338bee55cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "  pred_mask = pred_mask[..., tf.newaxis]\n",
    "  return pred_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e38ac02-3cd6-49d9-bde8-e527728d5123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(dataset=None, num=1):\n",
    "  if dataset:\n",
    "    for image, mask in dataset.take(num):\n",
    "      pred_mask = model.predict(image)\n",
    "      display([image[0], mask[0], create_mask(pred_mask)])\n",
    "  else:\n",
    "    mp = create_mask(model.predict(sample_image[tf.newaxis, ...]))\n",
    "    mpe = tf.keras.backend.eval(mp)\n",
    "    display([sample_image, sample_mask, mpe])\n",
    "show_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6f026-e808-4e11-a56d-b6af7bcd8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    clear_output(wait=True)\n",
    "    show_predictions()\n",
    "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4792dc2a-cbbb-4cd8-a509-042deee2b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "model_history = model.fit(train_ds, \n",
    "                   steps_per_epoch=int(np.ceil(num_train_examples / float(batch_size))),\n",
    "                   epochs=EPOCHS,\n",
    "                   validation_data=val_ds,\n",
    "                   validation_steps=int(np.ceil(num_val_examples / float(batch_size))),\n",
    "                   callbacks=[DisplayCallback()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca67f32c-676c-45ad-b570-e033cc52c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "epochs = range(EPOCHS)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'bo', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466d638-ebb1-4a13-9f1b-4d63a83644f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = os.path.join(\"\",'model_out/')\n",
    "if (not os.path.isdir(save_model_path)):\n",
    "  os.mkdir(save_model_path)\n",
    "model.save(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9ab64-1110-4943-9863-9c0b228c0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional, you can load the model from the saved version\n",
    "load_from_checkpoint = True\n",
    "if load_from_checkpoint == True:\n",
    "  model = tf.keras.models.load_model(save_model_path)\n",
    "else:\n",
    "  print(\"inferencing from in memory model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138897f-e909-43a0-8bcf-436efe3295d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(dataset=None, num=1):\n",
    "  if dataset:\n",
    "    for image, mask in dataset.take(num):\n",
    "      pred_mask = model.predict(image)\n",
    "      return pred_mask\n",
    "  else:\n",
    "    pred_mask = create_mask(model.predict(sample_image[tf.newaxis, ...]))\n",
    "    pred_mask = tf.keras.backend.eval(pred_mask)\n",
    "    return pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bec5f9-0948-46b3-9154-75bc6a3787fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_num = 1\n",
    "r_choices = np.random.choice(num_foreground_examples, 1)\n",
    "for i in range(0, display_num * 2, 2):\n",
    "  img_num = r_choices[i // 2]\n",
    "\n",
    "temp_ds = get_baseline_dataset(foreground_list_x[img_num:img_num+1], \n",
    "                               foreground_list_y[img_num:img_num+1],\n",
    "                               preproc_fn=test_preprocessing_fn,\n",
    "                               batch_size=1,\n",
    "                               shuffle=False)\n",
    "\n",
    "# Let's examine some of these augmented images\n",
    "\n",
    "iterator = iter(temp_ds)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "batch_of_imgs, label = next_element\n",
    "\n",
    "# Running next element in our graph will produce a batch of images\n",
    "\n",
    "sample_image, sample_mask = batch_of_imgs[0], label[0,:,:,:]\n",
    "\n",
    "# run and plot predicitions\n",
    "pred_mask = get_predictions()\n",
    "\n",
    "show_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae6b2e-fe8b-475e-a9cc-83fc9b73d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiled_prediction_dir = os.path.join(\"\",'predictions_test/')\n",
    "if not os.path.exists(tiled_prediction_dir):\n",
    "    os.makedirs(tiled_prediction_dir)\n",
    "    \n",
    "pred_masks = []\n",
    "true_masks = []\n",
    "\n",
    "for i in range(0, 20):\n",
    "    img_num = i\n",
    "\n",
    "    temp_ds = get_baseline_dataset(foreground_list_x[img_num:img_num+1], \n",
    "                                   foreground_list_y[img_num:img_num+1],\n",
    "                                   preproc_fn=tr_preprocessing_fn,\n",
    "                                   batch_size=1,\n",
    "                                   shuffle=False)\n",
    "\n",
    "    # Let's examine some of these augmented images\n",
    "\n",
    "    iterator = iter(temp_ds)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    batch_of_imgs, label = next_element\n",
    "\n",
    "    # Running next element in our graph will produce a batch of images\n",
    "\n",
    "    sample_image, sample_mask = batch_of_imgs[0], label[0,:,:,:]\n",
    "    sample_mask_int = tf.dtypes.cast(sample_mask, tf.int32)\n",
    "    true_masks.append(sample_mask_int)\n",
    "    print(foreground_list_y[img_num:img_num+1])\n",
    "    print(np.unique(sample_mask_int))\n",
    "\n",
    "    # run and plot predicitions\n",
    "\n",
    "    show_predictions()\n",
    "    pred_mask = get_predictions()\n",
    "    pred_masks.append(pred_mask)\n",
    "    \n",
    "    # save prediction images to file\n",
    "\n",
    "    filename_split = os.path.splitext(foreground_list_x[img_num]) \n",
    "    filename_zero, fileext = filename_split \n",
    "    basename = os.path.basename(filename_zero) \n",
    "    tf.keras.preprocessing.image.save_img(tiled_prediction_dir+'/'+basename+\".png\",pred_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3cdf48-ed3a-411f-a8b6-f2035674d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten our tensors and use scikit-learn to create a confusion matrix\n",
    "flat_preds = tf.reshape(pred_masks, [-1]) \n",
    "flat_truth = tf.reshape(true_masks, [-1]) \n",
    "cm = confusion_matrix(flat_truth, flat_preds, labels=list(range(OUTPUT_CHANNELS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388fcf88-fe92-45e0-840c-a4288f02c2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check values in predicted masks vs truth masks\n",
    "check_preds = tf.keras.backend.eval(flat_preds)\n",
    "check_truths = tf.keras.backend.eval(flat_truth)\n",
    "print(np.unique(check_preds), np.unique(check_truths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a12bd4-43a5-4f16-9f2e-ed4c6588d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "%matplotlib inline\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "# We want to show all ticks...\n",
    "ax.set(xticks=np.arange(cm.shape[1]),\n",
    "       yticks=np.arange(cm.shape[0]),\n",
    "       # ... and label them with the respective list entries\n",
    "       xticklabels=list(range(OUTPUT_CHANNELS)), yticklabels=list(range(OUTPUT_CHANNELS)),\n",
    "       title='Normalized Confusion Matrix',\n",
    "       ylabel='True label',\n",
    "       xlabel='Predicted label')\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "fmt = '.2f' #'d' # if normalize else 'd'\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], fmt),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "fig.tight_layout(pad=2.0, h_pad=2.0, w_pad=2.0)\n",
    "ax.set_ylim(len(classes)-0.5, -0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0675276-41fa-42c6-bd20-5a34ec63bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute f1 score\n",
    "f1_score(flat_truth, flat_preds, average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
